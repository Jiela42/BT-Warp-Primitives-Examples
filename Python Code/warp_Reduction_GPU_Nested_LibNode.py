from atexit import register
from random import random
import cupy
from dace.dtypes import StorageType
import numpy as np
import dace
from dace.sdfg import nodes
from dace.transformation.auto import auto_optimize as auto
from dace.transformation.dataflow import MapExpansion
from sympy import true

import WarpReductionExpansion

# I keep hardcoding the maxNumber of threads, we might wanna add a symbol instead

def gaussInit(a):
    for i in range(a.size):
        a[i] = i % 131          # The modulo ensures the numbers stay reasonably small and the prime makes accidental multiples a lot less likely

N = dace.symbol('N')
sz = 10000


# Careful: Number of threads aka BlockDim must be bigger than number of elements!
blockDim = 256
gridDim = 2048

# @dace.program
# def inner_product_python(A: dace.float64[N], B: dace.float64[N]):
#     return np.add.reduce(A * B)


# sdfg = inner_product_python.to_sdfg(simplify=True)
# for _, arr in sdfg.arrays.items():
#     if not arr.transient:
#         arr.storage = dace.StorageType.GPU_Global
# auto.auto_optimize(sdfg, dace.DeviceType.GPU)


A = np.ones((sz))
gaussInit(A)
gA = cupy.asarray(A)
B = np.ones((sz))
gaussInit(B)
gB = cupy.asarray(B)

r = cupy.array([float(0)])

# res = sdfg(A=gA, B=gB, N=sz)
# print("Autogenerated SDFG")
# print(res)

MaxTs = dace.symbol('MaxTs')
GridDim = dace.symbol('GridDim')
BlockDim = dace.symbol('BlockDim')
WarpSize = dace.symbol('WarpSize')


sdfg2 = dace.SDFG('reduction')
sdfg2.add_array('A', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_array('B', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_array('__return', shape=[1], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_symbol('MaxTs', stype= dace.int32)
sdfg2.add_symbol('GridDim', stype= dace.int32)
sdfg2.add_symbol('BlockDim', stype= dace.int32)
sdfg2.add_symbol('WarpSize', stype= dace.int32)

def Init(state, m):
    dst_node = state.add_write(m)
    init_t = state.add_tasklet('init_out', {}, {'__out'}, '__out = 0')
    
    state.add_edge (init_t, '__out', dst_node, None, dace.Memlet(data=m))
    

def GridStrideLoop(state, i1, i2, mS):

    init_tasklet = state.add_tasklet('init_sum', {}, {'__out'}, '__out = 0')
    sum_node = state.add_access(mS)
    state.add_edge(init_tasklet, '__out', sum_node, None, dace.Memlet.from_array(mS, state.parent.arrays[mS]))
    
    src_A = state.add_read(i1)
    src_B = state.add_read(i2)
    
    dst_node = state.add_access(mS)
    
    me,mx = state.add_map('gridSized_strides_map', dict(tId = 'i*BlockDim+j:N:MaxTs'))
    tasklet = state.add_tasklet('mult', {'in1', 'in2', '__in3'}, {'out'},  'out = in1 * in2 + __in3')
    
    state.add_memlet_path(src_A, me, tasklet, dst_conn='in1', memlet=dace.Memlet(data=i1, subset='tId'))
    state.add_memlet_path(src_B, me, tasklet, dst_conn='in2', memlet=dace.Memlet(data=i2, subset = 'tId'))
    state.add_memlet_path(sum_node, me, tasklet, dst_conn='__in3', memlet=dace.Memlet.from_array(mS, state.parent.arrays[mS]))
    state.add_memlet_path(tasklet, mx, dst_node, src_conn='out', memlet=dace.Memlet(data=mS, subset='0'))
    
def WriteBackState(state, mS, r):
    
    src_node = state.add_read(mS)
    dst_node = state.add_write(r)
    
    state.add_nedge(src_node, dst_node, dace.Memlet(f"{r}", wcr="lambda x, y: x + y"))      

##################################
# Create the GPU scheduleing state

gpuCallState = sdfg2.add_state()

me, mx = gpuCallState.add_map('GPU_map', {'i': '0:min(int_ceil(N, BlockDim), GridDim)', 'j': '0:BlockDim'})
me.map.schedule = dace.dtypes.ScheduleType.GPU_Device

##################################


##################################
# make the nested SDFG happen

gpu_sdfg = dace.SDFG('GPU_SDFG')

gpu_sdfg.add_array('sA', shape= [N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
gpu_sdfg.add_array('sB', shape= [N], dtype= dace.float64, storage=dace.StorageType.GPU_Global)
gpu_sdfg.add_array('sRes', shape=[1], dtype=dace.float64, storage=StorageType.GPU_Global)
gpu_sdfg.add_scalar('mySum', dtype=dace.float64, storage=StorageType.Register, transient=True)

# create inner sdfg
entry_state = gpu_sdfg.add_state('Entry_node')

init_sRes = gpu_sdfg.add_state('Init_sRes')
Init(init_sRes, 'sRes')

m_state= gpu_sdfg.add_state('Mult')
GridStrideLoop(m_state, 'sA', 'sB', 'mySum')

# The following commented out code is supposed to be turned into a library node

# gpu_sdfg.add_edge(entry_state, init_sRes, dace.InterstateEdge('j+i == 0'))
# gpu_sdfg.add_edge(entry_state, m_state, dace.InterstateEdge('j+i != 0'))

# gpu_sdfg.add_edge(init_sRes, m_state, dace.InterstateEdge())

# wwr_state= gpu_sdfg.add_state('WarpWise_Reduction')

# mSum = wwr_state.add_access('mySum')
# wtasklet = wwr_state.add_tasklet('warpwise_Reduction',
#                                  {}, {'__out'},
#                                  f"__out = __shfl_down_sync(0xFFFFFFFF, {mSum.data}, offset);",
#                                  dace.Language.CPP)
# wwr_state.add_edge(wtasklet, '__out', mSum, None, dace.Memlet(f"{mSum.data}", wcr="lambda x, y: x + y"))

# _, _, after_state = gpu_sdfg.add_loop(m_state, wwr_state, None, 'offset', 'WarpSize / 2', 'offset > 0', 'offset / 2')

# write_back_state = gpu_sdfg.add_state('Write_Back')
# WriteBackState(write_back_state, 'mySum', 'sRes')


# gpu_sdfg.add_edge(after_state, write_back_state, dace.InterstateEdge('j % WarpSize == 0'))
# gpu_sdfg.add_edge(after_state, random_end_state, dace.InterstateEdge('j % WarpSize != 0'))
# gpu_sdfg.add_edge(write_back_state, random_end_state, dace.InterstateEdge())

in_between = gpu_sdfg.add_state('Node_to_Expand')

random_end_state = gpu_sdfg.add_state('RandomEndState')

gpu_sdfg.add_edge(in_between, random_end_state, dace.InterstateEdge())

WarpReductionExpansion.apply_to(
    node=in_between,
    parent_state= None,
    parent_sdfg= gpu_sdfg,
    local_sum = 'mySum',
    local_result ='sRes')


##################################


##################################
# Make the dataflow between the states happen
da_whole_SDFG = gpuCallState.add_nested_sdfg(gpu_sdfg, sdfg2, {'sA', 'sB'}, {'sRes'})

Ain = gpuCallState.add_read('A')
Bin = gpuCallState.add_read('B')
ROut = gpuCallState.add_write('__return')

gpuCallState.add_memlet_path(Ain, me, da_whole_SDFG, memlet=dace.Memlet(data='A', subset='0: (min(int_ceil(N, BlockDim), GridDim) * BlockDim)'), dst_conn='sA')
gpuCallState.add_memlet_path(Bin, me, da_whole_SDFG, memlet=dace.Memlet(data='B', subset='0: (min(int_ceil(N, BlockDim), GridDim) * BlockDim)'), dst_conn='sB')
gpuCallState.add_memlet_path(da_whole_SDFG, mx, ROut, memlet=dace.Memlet(data='__return', subset='0'), src_conn='sRes')


sdfg2.apply_transformations_repeated(MapExpansion)

for n in gpuCallState.nodes():
    if isinstance(n, nodes.MapEntry) and "j" in n.map.params:
        n.map.schedule = dace.dtypes.ScheduleType.GPU_ThreadBlock
        


res2 = sdfg2(A=gA, B=gB, __return=r, N=sz, MaxTs = blockDim * gridDim, BlockDim = blockDim, GridDim = gridDim, WarpSize= 32)

print("Custom Sub SDFG")
print(res2)

# assert(np.allclose(res, res2))

##################
##################

# Old custom code version

# sdfgCustom = dace.SDFG('reduction')
# sdfgCustom.add_array('A', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
# sdfgCustom.add_array('B', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
# sdfgCustom.add_array('__return', shape=[1], dtype=dace.float64, storage=dace.StorageType.GPU_Global)


# def KernelCall(state):

#     tasklet_code = '''
#     if (i + j == 0) {
#         out[0] = double(0);
#     }
#     double sum = double(0);
#     for (int id = i * 256 + j; id < N; id += blockDim.x * gridDim.x) {
#         sum += in1[id] * in2[id];
#     }
#     for (int offset = warpSize/2; offset > 0; offset /= 2) {
#         sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
#     }
#     if (j % warpSize == 0) {
#         atomicAdd(out, sum);
#     }
#     '''

#     tasklet, me, mx = state.add_mapped_tasklet(
#         name='callingKernel',
#         map_ranges={'i': '0:min(int_ceil(N, 256), 2048)', 'j': '0:256'},            # i is blockIdx.x and j is threadIdx.x
#         inputs={'in1': dace.Memlet('A[0:N]'), 'in2': dace.Memlet('B[0:N]')},
#         outputs={'out': dace.Memlet('__return[0]')},
#         code=tasklet_code,
#         language=dace.dtypes.Language.CPP,
#         external_edges=True
#     )
#     out_conn = {'out': dace.pointer(dace.float64)}
#     tasklet.out_connectors = out_conn

#     me.map.schedule = dace.dtypes.ScheduleType.GPU_Device

# callState = sdfgCustom.add_state()
# KernelCall(callState)

# sdfgCustom.apply_transformations_repeated(MapExpansion)

# for n in callState.nodes():
#     if isinstance(n, nodes.MapEntry) and "j" in n.map.params:
#         n.map.schedule = dace.dtypes.ScheduleType.GPU_ThreadBlock
        
# resCustom = sdfgCustom(A=gA, B=gB, N=sz)

# print("Custom Code")
# print(resCustom)

# assert(np.allclose(res, resCustom))


