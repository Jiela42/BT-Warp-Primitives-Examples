from atexit import register
from random import random
import cupy
from dace.dtypes import StorageType
import numpy as np
import dace
from dace.sdfg import nodes
from dace.transformation.auto import auto_optimize as auto
from dace.transformation.dataflow import MapExpansion
from sympy import true
from dace.libraries.standard.nodes import Reduce

# I keep hardcoding the maxNumber of threads, we might wanna add a symbol instead

def gaussInit(a):
    for i in range(a.size):
        a[i] = i % 131          # The modulo ensures the numbers stay reasonably small and the prime makes accidental multiples a lot less likely

N = dace.symbol('N')
sz = 10000


# Careful: Number of threads aka BlockDim must be bigger than number of elements!
blockDim = 256
gridDim = 2048

@dace.program
def inner_product_python(A: dace.float64[N], B: dace.float64[N]):
    return np.add.reduce(A*B)


sdfg = inner_product_python.to_sdfg(simplify=True)
for _, arr in sdfg.arrays.items():
    if not arr.transient:
        arr.storage = dace.StorageType.GPU_Global
auto.auto_optimize(sdfg, dace.DeviceType.GPU)


A = np.ones((sz))
gaussInit(A)
gA = cupy.asarray(A)
B = np.ones((sz))
gaussInit(B)
gB = cupy.asarray(B)

r = cupy.array([float(0)])

res = sdfg(A=gA, B=gB, N=sz)
print("Autogenerated SDFG")
print(res)

MaxTs = dace.symbol('MaxTs')
GridDim = dace.symbol('GridDim')
BlockDim = dace.symbol('BlockDim')
WarpSize = dace.symbol('WarpSize')


sdfg2 = dace.SDFG('reduction')
sdfg2.add_array('A', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_array('B', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_array('__return', shape=[1], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
sdfg2.add_symbol('MaxTs', stype= dace.int32)
sdfg2.add_symbol('GridDim', stype= dace.int32)
sdfg2.add_symbol('BlockDim', stype= dace.int32)
sdfg2.add_symbol('WarpSize', stype= dace.int32)


somestate= sdfg2.add_state()

aA = somestate.add_access('A')
aret = somestate.add_access('__return')

red = Reduce(wcr = 'lambda a,b: a + b',
            identity = 0)

red.implementation = 'CUDA(shuffle)'


somestate.add_edge(aA, None, red, None, memlet= dace.Memlet.from_array('A', somestate.parent.arrays['A']))
somestate.add_edge(red, None, aret, None, dace.Memlet.from_array('__return', somestate.parent.arrays['__return']))
        


res2 = sdfg2(A=gA, B=gB, __return=r, N=sz, MaxTs = blockDim * gridDim, BlockDim = blockDim, GridDim = gridDim, WarpSize= 32)

print("Reduce_Library_Node")
print(res2)

# assert(np.allclose(res, res2))

##################
##################

# Old custom code version

# sdfgCustom = dace.SDFG('reduction')
# sdfgCustom.add_array('A', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
# sdfgCustom.add_array('B', shape=[N], dtype=dace.float64, storage=dace.StorageType.GPU_Global)
# sdfgCustom.add_array('__return', shape=[1], dtype=dace.float64, storage=dace.StorageType.GPU_Global)


# def KernelCall(state):

#     tasklet_code = '''
#     if (i + j == 0) {
#         out[0] = double(0);
#     }
#     double sum = double(0);
#     for (int id = i * 256 + j; id < N; id += blockDim.x * gridDim.x) {
#         sum += in1[id] * in2[id];
#     }
#     for (int offset = warpSize/2; offset > 0; offset /= 2) {
#         sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
#     }
#     if (j % warpSize == 0) {
#         atomicAdd(out, sum);
#     }
#     '''

#     tasklet, me, mx = state.add_mapped_tasklet(
#         name='callingKernel',
#         map_ranges={'i': '0:min(int_ceil(N, 256), 2048)', 'j': '0:256'},            # i is blockIdx.x and j is threadIdx.x
#         inputs={'in1': dace.Memlet('A[0:N]'), 'in2': dace.Memlet('B[0:N]')},
#         outputs={'out': dace.Memlet('__return[0]')},
#         code=tasklet_code,
#         language=dace.dtypes.Language.CPP,
#         external_edges=True
#     )
#     out_conn = {'out': dace.pointer(dace.float64)}
#     tasklet.out_connectors = out_conn

#     me.map.schedule = dace.dtypes.ScheduleType.GPU_Device

# callState = sdfgCustom.add_state()
# KernelCall(callState)

# sdfgCustom.apply_transformations_repeated(MapExpansion)

# for n in callState.nodes():
#     if isinstance(n, nodes.MapEntry) and "j" in n.map.params:
#         n.map.schedule = dace.dtypes.ScheduleType.GPU_ThreadBlock
        
# resCustom = sdfgCustom(A=gA, B=gB, N=sz)

# print("Custom Code")
# print(resCustom)

# assert(np.allclose(res, resCustom))


